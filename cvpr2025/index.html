<!DOCTYPE html><html lang="en-us" >


<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.5.0 for Hugo" />
  

  
  










  







  
  
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  

  
  
  
    
      
      <link rel="preload" as="style" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap">
      <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Montserrat:wght@400;700&family=Roboto+Mono&family=Roboto:wght@400;700&display=swap" media="print" onload="this.media='all'">
    
  

  
  
  
  
  
  

  

  
  
  
    
  
  <meta name="description" content="" />

  
  <link rel="alternate" hreflang="en-us" href="https://foundation-models-meet-embodied-agents.github.io./cvpr2025/" />

  
  
  
    <meta name="theme-color" content="#bbdefb" />
  

  
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.c7b8d9abd591ba2253ea42747e3ac3f5.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
      
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    

    
    
    
      
    
    
    
    
    
    <link rel="stylesheet" href="/css/libs/chroma/github-light.min.css" title="hl-light" media="print" onload="this.media='all'" disabled>
    <link rel="stylesheet" href="/css/libs/chroma/dracula.min.css" title="hl-dark" media="print" onload="this.media='all'" >

    
    
    
      <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.css" integrity="" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      
        
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.3008df2379f9291a2e82788480d8b32d.css" />

  



  


  


  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://foundation-models-meet-embodied-agents.github.io./cvpr2025/" />

  
  
  
  
  
  
  
  
    
    
  
  

  
  
    
    
  
  <meta property="twitter:card" content="summary" />
  
  <meta property="og:site_name" content="Foundation Models Meet Embodied Agents" />
  <meta property="og:url" content="https://foundation-models-meet-embodied-agents.github.io./cvpr2025/" />
  <meta property="og:title" content="Foundation Models Meet Embodied Agents" />
  <meta property="og:description" content="" /><meta property="og:image" content="https://foundation-models-meet-embodied-agents.github.io./media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://foundation-models-meet-embodied-agents.github.io./media/icon_hu127f241f63a982cda73a1c1b15888e16_9623_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
    
  

  



  

  

  
<link rel="preload" href="/webfonts/fa-solid-900.woff2" as="font" type="font/woff2" crossorigin="anonymous">
<link rel="preload" href="/webfonts/fa-brands-400.woff2" as="font" type="font/woff2" crossorigin="anonymous">


  <title>Foundation Models Meet Embodied Agents</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#navbar-main" class="page-wrapper  dark " data-wc-page-id="844b8a40823cdef67f5fe6bb7478109c" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.6a6813e7ed475370e052267c3a688edc.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<header class="header--fixed">
  <nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
    <div class="container-xl">

      
      <div class="d-none d-lg-inline-flex">
        <a class="navbar-brand" href="/">Foundation Models Meet Embodied Agents</a>
      </div>
      

      
      <button type="button" class="navbar-toggler" data-toggle="collapse"
              data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
      <span><i class="fas fa-bars"></i></span>
      </button>
      

      
      <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
        <a class="navbar-brand" href="/">Foundation Models Meet Embodied Agents</a>
      </div>
      

      
      
      <div class="navbar-collapse main-menu-item collapse justify-content-end" id="navbar-content">

        
        <ul class="navbar-nav d-md-inline-flex">
          

          
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Tutorials</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/aaai2025"><span>AAAI25</span></a>
                <a class="dropdown-item" href="/."><span>NAACL25</span></a>
                <a class="dropdown-item" href="/iccv2025_tutorial/" data-target="./iccv2025_tutorial"><span>ICCV25</span></a>
            </div>
          </li>

          
          

          
          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Workshops</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">
              
                <a class="dropdown-item" href="/cvpr2025"><span>CVPR25</span></a>
              
            </div>
          </li>

          <li class="nav-item dropdown">
            <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Challenges</span><span class="caret"></span>
            </a>
            <div class="dropdown-menu">

                  <a class="dropdown-item" href="/eai_challenge" data-target="./eai_challenge"><span>EAI Challenge @ NeurIPS 2025</span></a>
                  <a class="dropdown-item" href="/behavior_challenge" data-target="./behavior_challenge"><span>BEHAVIOR Challenge 2025</span></a>

            </div>
          </li>

        

          
        </ul>
      </div>

      <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

        
        

        
        

        
        
        

        
        

      </ul>

    </div>
  </nav>
</header>


  </div>

  <div class="page-body">
    
    
    

    
<span class="js-widget-page d-none"></span>





  
  
  
  





  

  
  
  
  

  
    
  

  

  
  
  
  

  
    
    
    
      
      
    
    
    
  

  
    
  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="intro" class="home-section wg-hero  d-flex align-items-center fullscreen"  >
   <div class="home-section-bg  bg-image parallax" style="background-color: white;background-image: url(&#39;https://foundation-models-meet-embodied-agents.github.io./media/backgrounds/robots_hu73593db37962f48d4f27238d67f9d2ab_657152_1920x1920_fit_q75_h2_lanczos_2.webp&#39;);filter: brightness(0.25);">
     
   </div>
    <div class="container">

    

      





    
      <h1 class="hero-title"><p style="color: white; font-size: 4rem; text-shadow: 0 0 2px black, 0 0 2px black, 0 0 2px black, 0 0 2px black;">Foundation Models Meet Embodied Agents</p>
<p style="color: white; font-size: 3rem; text-shadow: 0 0 2px black, 0 0 2px black, 0 0 2px black, 0 0 2px black;">@ CVPR 2025 Workshop</p>
</h1>
    

    
      <div class="hero-lead"><p>Wed June 11th, 2025, Room 214</p>
<p>at the Music City Center, Nashville TN</p>
<br>
<!-- Powered by the [Hugo Conference Theme](https://wowchemy.com/hugo-themes/) -->
</div>
    

    
    
      
      
      
        
      
      
      
      
      
        
      
    <p class="cta-btns">
      <a href="/cvpr2025/#call"  class="btn btn-primary btn-lg mb-md-1"><strong>Call For Papers</strong></a>

      
      
        
        
        
        
          
        
      <a href="/cvpr2025/#speakers"  class="hero-cta-alt btn-lg pl-md-4"><span style="color: white">Schedule</span> <i class="fas fa-angle-right"></i></a>
      
    </p>
    

    
    

  
  


    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="call" class="home-section wg-tickets  "  >
   <div class="home-section-bg " >
     
   </div>
    <div class="container">

    

      



<div class="row justify-content-center">
  
  <div class="section-heading col-12 mb-3 text-center">
    <h1 class="mb-0">Call for Papers</h1>
    
  </div>
  

  
  <div class="col-md-12">
    <p><em><strong>Submission Topics</strong></em></p>
<p>An embodied agent is a generalist agent that can take natural language instructions from humans and perform a wide range of tasks in diverse environments. Recent years have witnessed the emergence of Large Language Models as powerful tools for building Large Agent Models, which have shown remarkable success in supporting embodied agents for different abilities such as goal interpretation, subgoal decomposition, action sequencing, and transition modeling (causal transitions from preconditions to post-effects).</p>
<p>However, moving from Foundation Models to Embodied Agents poses significant challenges in understanding lower-level visual details, and long-horizon reasoning for reliable embodied decision-making.  We will cover the advances of the foundation models into Large Language Models Vision-Language Models, and Vision-Language-Action Models. In this workshop, we will comprehensively review existing paradigms for foundations for embodied agents, and focus on their different formulations based on the fundamental mathematical framework of robot learning, Markov Decision Process (MDP), and present a structured view to investigate the robot&rsquo;s decision-making process.</p>
<p>We welcome submissions on all topics related to Foundation Models and their interactions with Embodied Agents. We will also announce a Best Paper Award at our workshop.</p>
<p><em><strong>Submission Instructions</strong></em></p>
<p>We welcome submissions covering:</p>
<ul>
<li>Research papers: Long papers (8 pages) showcasing novel findings, methods, or theoretical advancements.</li>
<li>Short/Abstract papers: Features exploratory work (4 pages or 2 pages excluding references) that may be preliminary but presents innovative concepts, early results, or thought-provoking viewpoints that stimulate discussion and future work.</li>
<li>Position papers: Offer critical perspectives on trends and challenges within the field (no less than 8 pages).</li>
<li>Survey papers: Provide thorough reviews of specific topics, mapping the current research landscape and suggesting directions for future exploration (no less than 8 pages).</li>
</ul>
<p>All formats allow unlimited references and appendices.</p>
<p>Contributions will be non-archival but hosted on our workshop website, and thus dual submission is allowed where permitted by third parties. We welcome submissions that are under submission or accepted by other conferences. Please mention it in the last sentence of the paper abstract if your paper has been under submission or accepted by other conferences. Paper awards will prefer the original submissions.</p>
<p>Submissions should follow CVPR two-column style and be anonymous; see the <a href="https://cvpr.thecvf.com/Conferences/2025/AuthorGuidelines" target="_blank" rel="noopener">CVPR-25 author kit</a> for details. Please submit through <a href="https://openreview.net/group?id=thecvf.com/CVPR/2025/Workshop/FMEA" target="_blank" rel="noopener">OpenReview submission portal</a>.</p>
<p>We are looking for program committee members. Please sign up at <a href="https://forms.gle/bL17vmr7ZbybxERw9" target="_blank" rel="noopener">this form</a>.</p>

  </div>
  

</div>
<div class="row justify-content-center pt-3">
  <div class="col">
    
  </div>
</div>


    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  
    
    
    
      
      
    
    
    
  

  
    
  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="dates" class="home-section wg-blank dark fullscreen" style="padding: 80px 0 20px 0;" >
   <div class="home-section-bg  bg-image parallax" style="background-image: url(&#39;https://foundation-models-meet-embodied-agents.github.io./media/backgrounds/robots_hu73593db37962f48d4f27238d67f9d2ab_657152_1920x1920_fit_q75_h2_lanczos_2.webp&#39;);filter: brightness(0.25);">
     
   </div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
        
          <div class="section-heading col-12 mb-3 text-center">
            <h1 class="mb-0">Important Dates</h1>
            
          </div>
        
      
    

      



  <div class="col-12">
    <div style="padding: 1rem;">
<div class="col-12">
    <p>All deadlines are 11:59 pm UTC-12h (&ldquo;Anywhere on Earth&rdquo;).</p>
<table class="table">
    <tr>  <th>Submission Deadline</th>  <th><del>May 1st 2025</del> May 17th 2025  (23:59pm AoE)</th>  </tr>
    <tr>  <th>Call for Program Committee Members </th>  <th><del>May 1st 2025</del> May 17th 2025 (23:59pm AoE)</th>  </tr>
    <tr>
          <td data-table-dtype="text">Decision Notifications</td>
          <td data-table-dtype="text">May 25th 2025 (23:59pm AoE)</td>
    </tr>
    <tr>
          <td data-table-dtype="text">Camera-Ready Deadline (Non-Archival)</td>
          <td data-table-dtype="text">May 31st 2025 (23:59pm AoE)</td>
    </tr>
    <tr>
          <td data-table-dtype="text">Workshop Date</td>
          <td data-table-dtype="text">June 11th 2025</td>
    </tr>
</table>
</div>
</div>
  </div>



    
      </div>
    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  

  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="speakers" class="home-section wg-peopleme  "  >
   <div class="home-section-bg " >
     
   </div>
    <div class="container">

    

      









<div class="row justify-content-center people-widget">
  
  <div class="col-md-12 section-heading">
    <h1>Speakers</h1>
    
  </div>
  

  

  
  

  

  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/jitendramalik/avatar_hu1ec75d5004060ad1f5fa7efb43a4ba5d_4998099_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://people.eecs.berkeley.edu/~malik/">Jitendra Malik</a></h2>
      
      <h3>UC Berkeley</h3>
      
      
    </div>
  </div>
  
  
  
  
  <!-- <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/dieterfox/avatar_huc1928eb05d83709a9285a177279483cd_213936_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://homes.cs.washington.edu/~fox/">Dieter Fox</a></h2>
      
      <h3>University of Washington</h3>
      
      
    </div>
  </div> -->
  
  
  
  
  <!-- <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/xiaolongwang/avatar_hu2b9ac491d042884e1789b35a2382eb12_3532811_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://xiaolonw.github.io">Xiaolong Wang</a></h2>
      
      <h3>UC San Diego</h3>
      
      
    </div>
  </div> -->
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/yilundu/avatar_hu32b91c5cf1b65ae70803786fc8caedc7_1869356_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://yilundu.github.io">Yilun Du</a></h2>
      
      <h3>Google Deepmind; Harvard</h3>
      
      
    </div>
  </div>
  
  
  
  
  <!-- <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/dorsasadigh/avatar_hu5fa8d25af0c0bae91aae43a54647bc20_7069008_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://dorsa.fyi">Dorsa Sadigh</a></h2>
      
      <h3>Stanford University</h3>
      
      
    </div>
  </div> -->
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/michael-black/avatar_hu8373d410d40755b4ac1dcbc3725d5759_39375_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="http://ps.is.mpg.de">Michael Black</a></h2>
      
      <h3>MPI</h3>
      
      
    </div>
  </div>

  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/shuangli/avatar.jpg" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://shuangli59.github.io/">Shuang Li</a></h2>
      
      <h3>Stanford University</h3>
      
      
    </div>
  </div>


  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
    <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/ranjaykrishna/avatar.jpg" alt="Avatar">
  

  <div class="portrait-title">
    <h2><a href="https://www.ranjaykrishna.com/index.html">Ranjay Krishna</a></h2>
    
    <h3>University of Washington; AI2</h3>
    
    
  </div>
</div>



  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/katerinafragkiadaki/avatar.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://www.cs.cmu.edu/~katef/">Katerina Fragkiadaki</a></h2>
      
      <h3>Carnegie Mellon University</h3>
      
      
    </div>
  </div>
  
</div>


    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  
    
    
    
      
      
    
    
    
  

  
    
  

  
    
    
  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="schedule" class="home-section wg-blank dark fullscreen" style="padding: 20px 0 20px 0;" >
   <div class="home-section-bg  bg-image parallax" style="background-image: url(&#39;https://foundation-models-meet-embodied-agents.github.io./media/backgrounds/1689707021907_hua2d1c65aba91bcc21f702316314e8fc4_94670_1920x1920_fit_q75_h2_lanczos.webp&#39;);filter: brightness(0.2);">
     
   </div>
    <div class="container">

    
      <div class="row  justify-content-center">
      
        
          <div class="section-heading col-12 mb-3 text-center">
            <h1 class="mb-0">Schedule</h1>
            
          </div>
        
      
    

      



  <div class="col-12">
    <div class="col-12">
<table class="table">
  <thead>
    <tr>
      <th>Time</th>
      <th>Program</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>09:00-09:10</td>
      <td>Opening Remarks</td>
    </tr>
    <tr>
      <td>09:10-09:55</td>
      <td>Keynote Speech - <b>Ranjay Krishna: Behaviors & bodies: how they shape one another</b> [In person]
        <em class="abstract">
          Existing robot datasets contain a few embodiments but very few demonstrations per embodiment, resulting in no cross-embodiment generalization. In this talk, I will explore the possibility of using synthetic data to develop a foundation model for embodied navigation. I will first introduce our work on creating diverse synthetic environments from schools, to offices, to museums, to houses. Then, I will introduce a series of training algorithms that scale up navigation agents with synthetic training data, demonstrating sim-to-real transfer, and cross-embodiment generalization. Finally, I will highlight how embodiment-agnostic policies can enable tractable search algorithms, allowing us to explore the space of possible morphologies to identify effective ones that generalize to new unseen tasks.
        </em>
      </td>
    </tr>
    <tr>
      <td>09:55-10:30</td>
      <td>Oral Presentation<br>
        Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks<br>
        3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model <span style="color: red">(Best Paper)</span><br>
        One Demo Is All It Takes: Planning Domain Derivation with LLMs from a Single Demonstration</td>
    </tr>
    <tr>
      <td>10:30-11:00</td>
      <td>Coffee Break &amp; Poster at ExHall D</td>
    </tr>
    <tr>
      <td>11:00-11:45</td>
      <td>Keynote Speech - <b>Jitendra Malik</b> [Virtual]</td>
    </tr>
    <tr>
      <td>11:45-12:20</td>
      <td>Oral Presentation<br>
        Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations<br>
        SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation<br>
        VAGEN: Reinforcing Multi-Turn Visual State Reasoning for VLM Agents</td>
    </tr>
    <tr>
      <td>12:20-13:30</td>
      <td>Lunch Break + Student Mentoring Session</td>
    </tr>
    <tr>
      <td>13:30-14:15</td>
      <td>Keynote Speech - <b>Katerina Fragkiadaki: Goal-Driven Reasoning for Multimodal AI Agents</b> [In person]
        <em class="abstract">
          While large language models (LLMs) have shown impressive capabilities in text-based tasks, they remain limited in their ability to operate as autonomous agents capable of performing complex, real-world activities. Realizing applications such as web-based agents, augmented reality assistants, and personal robots requires moving beyond static knowledge retrieval toward systems with persistent memory, multimodal reasoning, and interactive learning. In this talk, I will present methods that address these key challenges: (1) scalable memory architectures that enable agents to plan and reason over extended time horizons; (2) human-in-the-loop learning frameworks that enhance internal reasoning and decision-making through exploration and feedback; (3) a reinforcement learning approach for grounded visual reasoning, where text thoughts are explicitly grounded on image evidence. Together, these contributions move us closer to building agents that can perceive, plan, adapt, and improve continuously in dynamic environments—bridging the gap between today’s static LLMs and tomorrow’s general-purpose autonomous systems.
        </em>
      </td>
    </tr>
    <tr>
      <td>14:15-15:00</td>
      <td>Keynote Speech - <b>Shuang Li: How Vision and Language Models Are Changing Decision-Making</b> [Virtual]
        <em class="abstract">
          Recent advances in vision and language models are transforming decision-making processes in robotics and automation. In this talk, we present several innovative frameworks that leverage language models as high-level planners to break down complex tasks into manageable subtasks, while employing video generation models as low-level controllers to execute actions in dynamic environments. We also introduce a novel unified video-and-action method that overcomes the limitations of directly applying video generation techniques to robotics and enables rapid policy inference. This integrated approach significantly enhances both the interpretability and efficiency of robotic systems.
        </em>
      </td>
    </tr>
    <tr>
      <td>15:00-15:10</td>
      <td>Abaka AI: Leading multimodal, text, and robotics data collection and annotation - Tom Tang</td>
    </tr>
    <tr>
      <td>15:10-15:30</td>
      <td>Coffee Break</td>
    </tr>
    <tr>
      <td>15:30-16:15</td>
      <td>Keynote Speech - <b>Yilun Du: Building Flexible Embodied Agents through Compositional World Modeling</b> [In person]
        <em class="abstract">
          A major bottleneck towards constructing intelligent embodied agents is the lack a of available data for all the settings the agent might find itself in. I’ll illustrate how we can operate well in such scenarios by building a “world model” then using inference/planning to solve new tasks the agent encounters. I’ll present a particular instantiation of a such a “world model”, using compositional energy functions, enabling models to generalize in areas we do not have data in. I illustrate a set of results using such an approach across perception, reasoning, and decision making.
        </em>
      </td>
    </tr>
    <tr>
      <td>16:15-17:00</td>
      <td>Keynote Speech - <b>Michael Black: Towards the 3D Human Foundation Agent</b> [In person]
        <em class="abstract">
          This talk will describe current progress on building a 3D Human Foundation Agent (HFA) that can perceive the world and the humans in it. The HFA is a digitally embodied agent that understands human behavior and responds to it using its “motor system” to translate its goals into 3D actions. The Human Foundation Agent must (1) perceive human movement in 3D, (2) understand the goals, implications, and emotions inherent in that movement, and (3) plan and generate natural motor activity to (4) drive a digital or physical embodiment that interacts with real or virtual humans in real or virtual 3D worlds. This talk will focus on current progress and the path to building HFAs through 3D human motion capture from video, synthetic training data, generative behavior modeling, AI-driven graphics, and large vision-language models that are fine-tuned to understand 3D humans. HFAs will radically change how people interact with machines. So much so that a child born today will have trouble imagining a world in which technology doesn’t understand their motions and behaviors.
        </em>
      </td>
    </tr>
    <tr>
      <td>17:00-17:05</td>
      <td>Best Paper Announcement</td>
    </tr>
    <tr>
      <td>17:05-17:15</td>
      <td>QA & Closing Remarks</td>
    </tr>
  </tbody>
</table>
</div>
<style>
  .table {
    width: 100%;
    border-collapse: collapse;
    margin-bottom: 1rem;
    font-size: 0.9rem;
  }
  .table td:first-child {
    white-space: nowrap;
    min-width: 85px;
  }
  .abstract {
    display: block;
    font-size: 0.75rem;
    line-height: 1.4;
    margin-top: 0.5rem;
  }
  @media screen and (max-width: 768px) {
    .table {
      font-size: 0.8rem;
    }
    .table td, .table th {
      padding: 0.5rem;
    }
    .abstract {
      font-size: 0.65rem;
    }
  }
</style><blockquote>
</blockquote>

  </div>



    
      </div>
    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  

  

  

  
  

  

  
  
<section id="accepted" class="home-section wg-tickets  "  >
  <div class="home-section-bg " >
    
  </div>
    <div class="container">

    

      



<div class="row justify-content-center">
  
  <div class="section-heading col-12 mb-3 text-center">
    <h1 class="mb-0">Accepted Papers</h1>
    
  </div>
  

  
  <div class="col-md-12">
    <p><em><strong>Optimus-1: Hybrid Multimodal Memory Empowered Agents Excel in Long-Horizon Tasks</strong></em> (oral)<br>
Zaijing Li, Yuquan Xie, Rui Shao, Gongwei Chen, Dongmei Jiang, Liqiang Nie</p>
<p><em><strong>3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model</strong></em> (oral)<br>
Wenbo Hu, Yining Hong, Yanjun Wang, Leison Gao, Zibu Wei, Xingcheng Yao, Nanyun Peng, Yonatan Bitton, Idan Szpektor, Kai-Wei Chang</p>
<p><em><strong>One Demo Is All It Takes: Planning Domain Derivation with LLMs from A Single Demonstration</strong></em> (oral)<br>
Jinbang Huang, Yixin Xiao, Zhanguang Zhang, Mark Coates, Jianye HAO, Yingxue Zhang</p>
<p><em><strong>Robotic Manipulation by Imitating Generated Videos Without Physical Demonstrations</strong></em> (oral)<br>
Shivansh Patel, Shraddhaa Mohan, Hanlin Mai, Unnat Jain, Svetlana Lazebnik, Yunzhu Li</p>
<p><em><strong>SAM2Act: Integrating Visual Foundation Model with A Memory Architecture for Robotic Manipulation</strong></em> (oral)<br>
Haoquan Fang, Markus Grotz, Wilbert Pumacay, Yi Ru Wang, Dieter Fox, Ranjay Krishna, Jiafei Duan</p>
<p><em><strong>VAGEN: Reinforcing Multi-Turn Visual State Reasoning for VLM Agents</strong></em> (oral)<br>
Kangrui Wang*, Pingyue Zhang*, Zihan Wang*, Yaning Gao*, Linjie Li*, Qineng Wang, Hanyang Chen, Yiping Lu, Zhengyuan Yang, Lijuan Wang, Ranjay Krishna, Jiajun Wu, Li Fei-Fei, Yejin Choi, Manling Li</p>
<p><em><strong>Embodied AI with Knowledge Graphs: Material-Aware Obstacle Handling for Autonomous Agents</strong></em><br>
Ayush Bheemaiah, Seungyong Yang</p>
<p><em><strong>Beyond Needle(s) in the Embodied Haystack: Environment, Architecture, and Training Considerations for Long Context Reasoning</strong></em><br>
Bosung Kim, Prithviraj Ammanabrolu</p>
<p><em><strong>Visual Planning: Let's Think Only with Images</strong></em><br>
Yi Xu, Chengzu Li, Han Zhou, Xingchen Wan, Caiqi Zhang, Anna Korhonen, Ivan Vulić</p>
<p><em><strong>Model-Based Policy Adaptation for Closed-Loop End-to-End Autonomous Driving</strong></em><br>
Haohong Lin, Yunzhi Zhang, Wenhao Ding, Jiajun Wu, Ding Zhao</p>
<p><em><strong>Slot-Level Robotic Placement via Visual Imitation from Single Human Video</strong></em><br>
Dandan Shan, Kaichun Mo, Wei Yang, Yu-Wei Chao, David Fouhey, Dieter Fox, Arsalan Mousavian</p>
<p><em><strong>Interactive Post-Training for Vision-Language-Action Models</strong></em><br>
Shuhan Tan, Kairan Dou, Yue Zhao, Philipp Kraehenbuehl</p>
<p><em><strong>Human-like Navigation in a World Built for Humans</strong></em><br>
Bhargav Chandaka, Gloria X. Wang, Haozhe Chen, Henry Che, Albert J. Zhai, Shenlong Wang</p>
<p><em><strong>AetherVision-Bench: An Open-Vocabulary RGB-Infrared Benchmark for Multi-Angle Segmentation across Aerial and Ground Perspectives</strong></em><br>
Aniruddh Sikdar, Aditya Gandhamal, Suresh Sundaram</p>
<p><em><strong>Episodic Memory Banks for Lifelong Robot Learning: A Case Study Focusing on Household Navigation and Manipulation</strong></em><br>
Zichao Li</p>
<p><em><strong>TRAVEL: Training-Free Retrieval and Alignment for Vision-and-Language Navigation</strong></em><br>
Navid Rajabi, Jana Kosecka</p>
<p><em><strong>Uncertainty Modeling in Autonomous Vehicle Trajectory Prediction: A Comprehensive Survey</strong></em><br>
Siddharth Raina, Jeshwanth Challagundla, Mantek Singh</p>
<p><em><strong>Mem2Ego: Empowering Vision-Language Models with Global-to-Ego Memory for Long-Horizon Embodied Navigation</strong></em><br>
Lingfeng Zhang, Yuecheng Liu, Zhanguang Zhang, Matin Aghaei, Yaochen Hu, Hongjian Gu, Mohammad Ali Alomrani, David Gamaliel Arcos Bravo, Raika Karimi, Atia Hamidizadeh, Haoping Xu, Guowei Huang, zhanpeng zhang, Tongtong Cao, Weichao Qiu, Xingyue Quan, Jianye HAO, Yuzheng Zhuang, Yingxue Zhang</p>
<p><em><strong>SemNav: A Model-Based Planner for Zero-Shot Object Goal Navigation Using Vision-Foundation Models</strong></em><br>
Arnab Debnath, Gregory J. Stein, Jana Kosecka</p>
<p><em><strong>ZeroMimic: Distilling Robotic Manipulation Skills from Web Videos</strong></em><br>
Junyao Shi, Zhuolun Zhao, Tianyou Wang, Ian Pedroza, Amy Luo, Jie Wang, Yecheng Jason Ma, Dinesh Jayaraman</p>
<p><em><strong>Scalable Decision-Making in Stochastic Environments through Learned Temporal Abstraction</strong></em><br>
Baiting Luo, Abhishek Dubey, Ayan Mukhopadhyay</p>
  </div>
  

</div>
<div class="row justify-content-center pt-3">
  <div class="col">
    
  </div>
</div>


    

    </div>
  </section>
  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="organization" class="home-section wg-peopleme  "  >
   <div class="home-section-bg " >
     
   </div>
    <div class="container">

    

      









<div class="row justify-content-center people-widget">
  
  <div class="col-md-12 section-heading">
    <h1>Organizers</h1>
    
  </div>
  

  

  
  

  
  <div class="col-md-12">
    <h2 class="mb-4">Organizer Committee @ CVPR25</h2>
  </div>
  

  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/manling-li/avatar_hudf540693b64f9a6a2d70f06d58229227_7031871_270x270_fill_q75_lanczos_center.jpg" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://limanling.github.io/">Manling Li</a></h2>
      
      <h3>Northwestern University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/weiyuliu/avatar_hu4cd356c4970b3090fda4d834607f9e2e_20222323_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://www.weiyuliu.com">Weiyu Liu</a></h2>
      
      <h3>Stanford University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/ruohanzhang/avatar_hued8767adb6e885916276fc164ddb5f95_5880877_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://ai.stanford.edu/~zharu/">Ruohan Zhang</a></h2>
      
      <h3>Stanford University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/yunzhuli/avatar_huc67ac1113fa0be91af69b0ea0b88a67a_39550_270x270_fill_q75_lanczos_center.jpg" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://yunzhuli.github.io">Yunzhu Li</a></h2>
      
      <h3>Columbia University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/zihan-wang/avatar_huf668f7b99c22485b3fbc4101777f8cfb_669867_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://zihanwang314.github.io">Zihan Wang</a></h2>
      
      <h3>Northwestern University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/bryanzhou/avatar_hufd16622fd8f5cc15cec4f32980a96646_846927_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://yu-bryan-zhou.github.io">Bryan Yu Zhou</a></h2>
      
      <h3>UCLA</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/jiayuanmao/avatar_hucf95170c66de7bab9a1b26ce3fa4d9f8_47317_270x270_fill_q75_lanczos_center.jpg" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://jiayuanm.com">Jiayuan Mao</a></h2>
      
      <h3>MIT</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/qinengwang/avatar_hu1b80a880db9cdcf0e538966c6b9ad1d2_67041_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://qinengwang-aiden.github.io">Qineng Wang</a></h2>
      
      <h3>Northwestern University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/wenlonghuang/avatar_hu0f844cf00768015da650b6fc6dab2316_97243_270x270_fill_q75_lanczos_center.jpg" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://wenlong.page">Wenlong Huang</a></h2>
      
      <h3>Stanford University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  

  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/xiaohanzhang/avatar.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://keke-220.github.io/">Xiaohan Zhang</a></h2>
      
      <h3>Boston Dynamics AI Institute</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/jiajunwu/avatar_huf3b08ed51519a61bbca87677ca1d0729_228027_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://jiajunwu.com">Jiajun Wu</a></h2>
      
      <h3>Stanford University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  

  
  <div class="col-md-12">
    <h2 class="mb-4">Steering Committee @ CVPR25</h2>
  </div>
  

  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/yonatanbisk/avatar_hua1a97b34542bc56a2251a9017c05550c_331917_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://talkingtorobots.com/yonatanbisk.html">Yonatan Bisk</a></h2>
      
      <h3>CMU</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/shenlongwang/avatar_hu58380ffc0f075edc60b5cefec2e8c0be_1835958_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://shenlong.web.illinois.edu">Shenlong Wang</a></h2>
      
      <h3>UIUC</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/yejincho/avatar_hudea140e4acd2a8cb523f99be36df2da1_7069291_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://yejinc.github.io">Yejin Choi</a></h2>
      
      <h3>NVIDIA, Stanford University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
  
  
  <div class="col-12 col-sm-auto people-person">
    
    
      
      
    
    
      
      <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/feifeili/avatar_hu05a4820cb0b2258f2bc3d495b198cc2f_165269_270x270_fill_lanczos_center_3.png" alt="Avatar">
    

    <div class="portrait-title">
      <h2><a href="https://profiles.stanford.edu/fei-fei-li">Fei-Fei Li</a></h2>
      
      <h3>Stanford University</h3>
      <ul class="network-icon" aria-hidden="true">
  
</ul>

      
    </div>
  </div>
  
  
</div>


    

    </div>
  </section>

  

  
  
  
  

  

  

  
  
  
  

  
    
    
    
      
      
    
    
    
  

  
    
  

  

  
  

  

  
  

  
  
  

  
  
  
  
  

  
  

  

  
  

  
  

  
  <section id="sponsors" class="home-section wg-peopleme">
    <div class="home-section-bg">
    </div>
    <div class="container">
      <div class="row justify-content-center people-widget">
        <div class="col-md-12 section-heading">
          <h1>Sponsors</h1>
        </div>
        <div class="col-12 col-sm-auto people-person">
          <img width="270" height="270" loading="lazy" class="avatar avatar-circle" src="/authors/abaka/avatar.png" alt="Avatar">
          <div class="portrait-title">
            <h2><a href="https://www.abaka.ai/">abaka.ai</a></span></h2>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section id="contact" class="home-section wg-tickets dark">
   <div class="home-section-bg  bg-image parallax" style="background-image: url(&#39;https://foundation-models-meet-embodied-agents.github.io./media/backgrounds/robots_hu73593db37962f48d4f27238d67f9d2ab_657152_1920x1920_fit_q75_h2_lanczos_2.webp&#39;);filter: brightness(0.25);">
     
   </div>
    <div class="container">

    

      



<div class="row justify-content-center">
  

  

</div>
<div class="row justify-content-center pt-3">
  <div class="col">
    
    <div class="row pb-3">
      <div class="col-12 col-md-4">
        <div class="section-subheading">Contact</div>
      </div>
      <div class="col-12 col-md-6">
        <p>Please email <a href="mailto:cvpr2025-foundationmodel-embodied@googlegroups.com">cvpr2025-foundationmodel-embodied@googlegroups.com</a> if you have any questions.</p>
      </div>
      <div class="col-12 col-md-2">
        <a href="mailto:cvpr2025-foundationmodel-embodied@googlegroups.com" target="_blank" rel="noreferrer noopener" class="btn btn-primary btn-lg w-100" style="color: black;">Email Us</a>
      </div>
    </div>
    
  </div>
</div>


    

    </div>
  </section>






  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  

  
  






  
  
  

  
  
    
  
  
    
  

  

  
  <p class="powered-by copyright-license-text">
    © 2025 Me. This work is licensed under <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank">CC BY NC ND 4.0</a>
  </p>
  

  <p class="powered-by footer-license-icons">
    <a href="https://creativecommons.org/licenses/by-nc-nd/4.0" rel="noopener noreferrer" target="_blank" aria-label="Creative Commons">
      <i class="fab fa-creative-commons fa-2x" aria-hidden="true"></i>
      <i class="fab fa-creative-commons-by fa-2x" aria-hidden="true"></i>
      
        <i class="fab fa-creative-commons-nc fa-2x" aria-hidden="true"></i>
      
      
        <i class="fab fa-creative-commons-nd fa-2x" aria-hidden="true"></i>
      
    </a>
  </p>




  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-themes" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

      

    
    <script src="/js/vendor-bundle.min.d26509351aa0ff874abbee824e982e9b.js"></script>

    
    
    
      

      
      

      

    

    
    
    
      <script src="https://cdn.jsdelivr.net/npm/leaflet@1.7.1/dist/leaflet.min.js" integrity="" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdn.jsdelivr.net/gh/krisk/Fuse@v3.2.1/dist/fuse.min.js" integrity="sha512-o38bmzBGX+hD3JHWUFCDA09btWaqrNmoJ3RXLlrysA7PP01Kgs4UlE4MhelE1v5dJR3+cxlR4qQlotsW7jKsnw==" crossorigin="anonymous"></script>
        <script src="https://cdn.jsdelivr.net/gh/julmot/mark.js@8.11.1/dist/jquery.mark.min.js" integrity="sha512-mhbv5DqBMgrWL+32MmsDOt/OAvqr/cHimk6B8y/bx/xS88MVkYGPiVv2ixKVrkywF2qHplNRUvFsAHUdxZ3Krg==" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    

    
    
    
    <script id="page-data" type="application/json">{"use_headroom":true}</script>

    
    
      <script src="/js/wowchemy-headroom.c251366b4128fd5e6b046d4c97a62a51.js" type="module"></script>
    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.992ab4bf929c75fb2aff9ec73febac85.js"></script>

    
    
      <script src="/js/wowchemy-map.a26e9d2f7238ba5b868384f1c5bc6477.js" type="module"></script>
    
    
    
    






</body>
</html>
